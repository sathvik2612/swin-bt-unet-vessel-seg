{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e6b99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras import backend as K\n",
    "from hausdorff import hausdorff_distance\n",
    "#import lr_scheduler\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread,imshow\n",
    "from skimage.morphology import label\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, concatenate\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from keras_vision_transformer import swin_layers\n",
    "from keras_vision_transformer import transformer_layers\n",
    "from keras_vision_transformer import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce30d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH=\"DRIVE/training/\"\n",
    "TEST_PATH=\"DRIVE/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8546bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "train_ids=next(os.walk(TRAIN_PATH+'images/'))[2][:]\n",
    "test_ids=next(os.walk(TEST_PATH+'images/'))[2][:]\n",
    "print(len(train_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b02ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = IMG_HEIGHT = 256\n",
    "input_width = IMG_WIDTH = 256\n",
    "IMG_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38b822e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing training images and masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]C:\\Users\\manda\\AppData\\Local\\Temp\\ipykernel_24880\\4178424171.py:12: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
      "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: 'C:\\Users\\manda\\Desktop\\aaa\\DRIVE\\training\\1st_manual\\21_training_manual.gif'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((IMG_HEIGHT, IMG_WIDTH, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool)\n\u001b[0;32m     13\u001b[0m mask_path \u001b[38;5;241m=\u001b[39m TRAIN_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1st_manual/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m id_[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_manual.gif\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 14\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(mask, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m mask \u001b[38;5;241m=\u001b[39m resize(mask, (input_width, input_height), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, preserve_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\io\\_io.py:53\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[0;32m     50\u001b[0m         plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtifffile\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_or_url_context(fname) \u001b[38;5;28;01mas\u001b[39;00m fname:\n\u001b[1;32m---> 53\u001b[0m     img \u001b[38;5;241m=\u001b[39m call_plugin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimread\u001b[39m\u001b[38;5;124m'\u001b[39m, fname, plugin\u001b[38;5;241m=\u001b[39mplugin, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mplugin_args)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(img, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\io\\manage_plugins.py:207\u001b[0m, in \u001b[0;36mcall_plugin\u001b[1;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find the plugin \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    205\u001b[0m                            (plugin, kind))\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\io\\_plugins\\imageio_plugin.py:10\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(imageio_imread)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(imageio_imread(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\imageio\\core\\functions.py:265\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(uri, format, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid keyword argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperhaps you mean \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpilmode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    262\u001b[0m     )\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Get reader and read first\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m reader \u001b[38;5;241m=\u001b[39m read(uri, \u001b[38;5;28mformat\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mget_data(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\imageio\\core\\functions.py:172\u001b[0m, in \u001b[0;36mget_reader\u001b[1;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m\"\"\" get_reader(uri, format=None, mode='?', **kwargs)\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03mReturns a :class:`.Reader` object which can be used to read data\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m    to see what arguments are available for a particular format.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Create request object\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m request \u001b[38;5;241m=\u001b[39m Request(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Get format\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\imageio\\core\\request.py:124\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[1;34m(self, uri, mode, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequest requires mode[1] to be in \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miIvV?\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Parse what was given\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Set extension\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\imageio\\core\\request.py:260\u001b[0m, in \u001b[0;36mRequest._parse_uri\u001b[1;34m(self, uri)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_read_request:\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m# Reading: check that the file exists (but is allowed a dir)\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(fn):\n\u001b[1;32m--> 260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m fn)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;66;03m# Writing: check that the directory to write to does exist\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     dn \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(fn)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: No such file: 'C:\\Users\\manda\\Desktop\\aaa\\DRIVE\\training\\1st_manual\\21_training_manual.gif'"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n",
    "Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.float32)\n",
    "X_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n",
    "Y_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.float32)\n",
    "\n",
    "print('Resizing training images and masks')\n",
    "for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):   \n",
    "    path = TRAIN_PATH + 'images/' + id_\n",
    "    img = imread(path)[:,:,:IMG_CHANNELS]  \n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    X_train[n] = img #/ 255. #Fill empty X_train with values from img\n",
    "    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "    mask_path = TRAIN_PATH + '1st_manual/' + id_[:-4] + '_manual.gif'\n",
    "    mask = imread(mask_path)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    mask = resize(mask, (input_width, input_height), mode='constant', preserve_range=True)\n",
    "    mask = mask/255\n",
    "    Y_train[n][mask > 0.] = 1.\n",
    "\n",
    "print('Resizing test images and masks')\n",
    "for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):   \n",
    "    path = TEST_PATH + 'images/' + id_\n",
    "    img = imread(path)[:,:,:IMG_CHANNELS]  \n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    X_test[n] = img #/ 255. #Fill empty X_train with values from img\n",
    "    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "    mask_path = TEST_PATH + '1st_manual/' + id_[:-4] + '_manual.gif'\n",
    "    mask = imread(mask_path)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    mask = resize(mask, (input_width, input_height), mode='constant', preserve_range=True)\n",
    "    mask = mask/255\n",
    "    Y_test[n][mask > 0.] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d890c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715af0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix=0\n",
    "imshow(X_train[ix].astype('int'))\n",
    "plt.show()\n",
    "imshow(np.squeeze(Y_train[ix].astype('int')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,30))\n",
    "x, y = 5, 4\n",
    "count=1\n",
    "for i in range(y):\n",
    "    for j in range(x):\n",
    "        plt.subplot(y*2, x, i*2*x+j+1)\n",
    "        pos = i+j\n",
    "        plt.imshow(X_train[pos].astype('int'))\n",
    "        plt.title('Image {}'.format(count))\n",
    "        plt.axis('off')\n",
    "        plt.subplot(y*2, x, (i*2+1)*x+j+1)\n",
    "        plt.imshow(np.squeeze(Y_train[pos].astype('int')))\n",
    "        plt.title('Mask {}'.format(count))\n",
    "        plt.axis('off')\n",
    "        count = count + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98435d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expend_as(tensor, rep):\n",
    "     return Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n",
    "                          arguments={'repnum': rep})(tensor)\n",
    "\n",
    "def double_conv_layer(x, filter_size, size, dropout, batch_norm=False):\n",
    "    axis = 3\n",
    "    conv = SeparableConv2D(size, (filter_size, filter_size), padding='same')(x)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = SeparableConv2D(size, (filter_size, filter_size), padding='same')(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    if dropout > 0:\n",
    "        conv = Dropout(dropout)(conv)\n",
    "\n",
    "    shortcut = Conv2D(size, kernel_size=(1, 1), padding='same')(x)\n",
    "    if batch_norm is True:\n",
    "        shortcut = BatchNormalization(axis=axis)(shortcut)\n",
    "\n",
    "    res_path = add([shortcut, conv])\n",
    "    return res_path\n",
    "\n",
    "def encoder(inputs):\n",
    "    num_filters = [16, 32, 64, 128]\n",
    "    skip_connections = []\n",
    "    x = inputs\n",
    "\n",
    "    for i, f in enumerate(num_filters):\n",
    "        a = double_conv_layer(x, 3, f, 0.1, True)\n",
    "        skip_connections.append(a)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(a)\n",
    "    \n",
    "    return x, skip_connections\n",
    "\n",
    "def bottleneck(inputs):\n",
    "    x = inputs\n",
    "    f = 256\n",
    "    \n",
    "    x3 = double_conv_layer(x, 3, f, 0.1, True)\n",
    "    \n",
    "    return x3\n",
    "\n",
    "def decoder(inputs, skip_connections):\n",
    "    num_filters = [128, 64, 32, 16]\n",
    "    skip_connections.reverse()\n",
    "    x = inputs\n",
    "    batch_norm = True\n",
    "    \n",
    "    for i, f in enumerate(num_filters):\n",
    "        \n",
    "        x_up = UpSampling2D(size=(2, 2), data_format=\"channels_last\")(x)\n",
    "        x_att = concatenate([x_up, skip_connections[i]], axis=-1)\n",
    "        \n",
    "        x = double_conv_layer(x_att, 3, f, 0.1, True)\n",
    "    return x\n",
    "\n",
    "def output(inputs):\n",
    "    x = Conv2D(1, kernel_size=(1,1))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a641c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swin_transformer_stack(X, stack_num, embed_dim, num_patch, num_heads, window_size, num_mlp, shift_window=True, name=''):\n",
    "    '''\n",
    "    Stacked Swin Transformers that share the same token size.\n",
    "    \n",
    "    Alternated Window-MSA and Swin-MSA will be configured if `shift_window=True`, Window-MSA only otherwise.\n",
    "    *Dropout is turned off.\n",
    "    '''\n",
    "    # Turn-off dropouts\n",
    "    mlp_drop_rate = 0 # Droupout after each MLP layer\n",
    "    attn_drop_rate = 0 # Dropout after Swin-Attention\n",
    "    proj_drop_rate = 0 # Dropout at the end of each Swin-Attention block, i.e., after linear projections\n",
    "    drop_path_rate = 0 # Drop-path within skip-connections\n",
    "    \n",
    "    qkv_bias = True # Convert embedded patches to query, key, and values with a learnable additive value\n",
    "    qk_scale = None # None: Re-scale query based on embed dimensions per attention head # Float for user specified scaling factor\n",
    "    \n",
    "    if shift_window:\n",
    "        shift_size = window_size // 2\n",
    "    else:\n",
    "        shift_size = 0\n",
    "    \n",
    "    for i in range(stack_num):\n",
    "    \n",
    "        if i % 2 == 0:\n",
    "            shift_size_temp = 0\n",
    "        else:\n",
    "            shift_size_temp = shift_size\n",
    "\n",
    "        X = swin_layers.SwinTransformerBlock(dim=embed_dim, \n",
    "                                             num_patch=num_patch, \n",
    "                                             num_heads=num_heads, \n",
    "                                             window_size=window_size, \n",
    "                                             shift_size=shift_size_temp, \n",
    "                                             num_mlp=num_mlp, \n",
    "                                             qkv_bias=qkv_bias, \n",
    "                                             qk_scale=qk_scale,\n",
    "                                             mlp_drop=mlp_drop_rate, \n",
    "                                             attn_drop=attn_drop_rate, \n",
    "                                             proj_drop=proj_drop_rate, \n",
    "                                             drop_path_prob=drop_path_rate, \n",
    "                                             name='name{}'.format(i))(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swin_unet_2d_base(input_tensor, filter_num_begin, depth, stack_num_down, stack_num_up, \n",
    "                      patch_size, num_heads, window_size, num_mlp, shift_window=True, name='swin_unet'):\n",
    "    '''\n",
    "    The base of Swin-UNET.\n",
    "    \n",
    "    The general structure:\n",
    "    \n",
    "    1. Input image --> a sequence of patches --> tokenize these patches\n",
    "    2. Downsampling: swin-transformer --> patch merging (pooling)\n",
    "    3. Upsampling: concatenate --> swin-transfprmer --> patch expanding (unpooling)\n",
    "    4. Model head\n",
    "    \n",
    "    '''\n",
    "    # Compute number be patches to be embeded\n",
    "    input_size = input_tensor.shape.as_list()[1:]\n",
    "    num_patch_x = input_size[0]//patch_size[0]\n",
    "    num_patch_y = input_size[1]//patch_size[1]\n",
    "    \n",
    "    # Number of Embedded dimensions\n",
    "    embed_dim = filter_num_begin\n",
    "    \n",
    "    depth_ = depth\n",
    "    \n",
    "    X_skip = []\n",
    "\n",
    "    X = input_tensor\n",
    "    \n",
    "    # Patch extraction\n",
    "    X = transformer_layers.patch_extract(patch_size)(X)\n",
    "\n",
    "    # Embed patches to tokens\n",
    "    X = transformer_layers.patch_embedding(num_patch_x*num_patch_y, embed_dim)(X)\n",
    "    \n",
    "    # The first Swin Transformer stack\n",
    "    X = swin_transformer_stack(X, \n",
    "                               stack_num=stack_num_down, \n",
    "                               embed_dim=embed_dim, \n",
    "                               num_patch=(num_patch_x, num_patch_y), \n",
    "                               num_heads=num_heads[0], \n",
    "                               window_size=window_size[0], \n",
    "                               num_mlp=num_mlp, \n",
    "                               shift_window=shift_window, \n",
    "                               name='{}_swin_down0'.format(name))\n",
    "    X_skip.append(X)\n",
    "    \n",
    "    # Downsampling blocks\n",
    "    for i in range(depth_-1):\n",
    "        \n",
    "        # Patch merging\n",
    "        X = transformer_layers.patch_merging((num_patch_x, num_patch_y), embed_dim=embed_dim, name='down{}'.format(i))(X)\n",
    "        \n",
    "        # update token shape info\n",
    "        embed_dim = embed_dim*2\n",
    "        num_patch_x = num_patch_x//2\n",
    "        num_patch_y = num_patch_y//2\n",
    "        \n",
    "        # Swin Transformer stacks\n",
    "        X = swin_transformer_stack(X, \n",
    "                                   stack_num=stack_num_down, \n",
    "                                   embed_dim=embed_dim, \n",
    "                                   num_patch=(num_patch_x, num_patch_y), \n",
    "                                   num_heads=num_heads[i+1], \n",
    "                                   window_size=window_size[i+1], \n",
    "                                   num_mlp=num_mlp, \n",
    "                                   shift_window=shift_window, \n",
    "                                   name='{}_swin_down{}'.format(name, i+1))\n",
    "        \n",
    "        # Store tensors for concat\n",
    "        X_skip.append(X)\n",
    "        \n",
    "    # reverse indexing encoded tensors and hyperparams\n",
    "    X_skip = X_skip[::-1]\n",
    "    num_heads = num_heads[::-1]\n",
    "    window_size = window_size[::-1]\n",
    "    \n",
    "    # upsampling begins at the deepest available tensor\n",
    "    X = X_skip[0]\n",
    "    \n",
    "    # other tensors are preserved for concatenation\n",
    "    X_decode = X_skip[1:]\n",
    "    \n",
    "    depth_decode = len(X_decode)\n",
    "    \n",
    "    for i in range(depth_decode):\n",
    "        \n",
    "        # Patch expanding\n",
    "        X = transformer_layers.patch_expanding(num_patch=(num_patch_x, num_patch_y), \n",
    "                                               embed_dim=embed_dim, \n",
    "                                               upsample_rate=2, \n",
    "                                               return_vector=True)(X)\n",
    "        \n",
    "\n",
    "        # update token shape info\n",
    "        embed_dim = embed_dim//2\n",
    "        num_patch_x = num_patch_x*2\n",
    "        num_patch_y = num_patch_y*2\n",
    "        \n",
    "        # Concatenation and linear projection\n",
    "        X = concatenate([X, X_decode[i]], axis=-1, name='{}_concat_{}'.format(name, i))\n",
    "        X = Dense(embed_dim, use_bias=False, name='{}_concat_linear_proj_{}'.format(name, i))(X)\n",
    "        \n",
    "        # Swin Transformer stacks\n",
    "        X = swin_transformer_stack(X, \n",
    "                                   stack_num=stack_num_up, \n",
    "                                   embed_dim=embed_dim, \n",
    "                                   num_patch=(num_patch_x, num_patch_y), \n",
    "                                   num_heads=num_heads[i], \n",
    "                                   window_size=window_size[i], \n",
    "                                   num_mlp=num_mlp, \n",
    "                                   shift_window=shift_window, \n",
    "                                   name='{}_swin_up{}'.format(name, i))\n",
    "        \n",
    "    # The last expanding layer; it produces full-size feature maps based on the patch size\n",
    "    # !!! <--- \"patch_size[0]\" is used; it assumes patch_size = (size, size)\n",
    "    \n",
    "    X = transformer_layers.patch_expanding(num_patch=(num_patch_x, num_patch_y), \n",
    "                                           embed_dim=embed_dim, \n",
    "                                           upsample_rate=patch_size[0], \n",
    "                                           return_vector=False)(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583551e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_tensor, filter_num_begin, depth, stack_num_down, stack_num_up, \n",
    "                      patch_size, num_heads, window_size, num_mlp, shift_window=True, name='swin_unet_encoder'):\n",
    "    '''\n",
    "    The base of Swin-UNET.\n",
    "    \n",
    "    The general structure:\n",
    "    \n",
    "    1. Input image --> a sequence of patches --> tokenize these patches\n",
    "    2. Downsampling: swin-transformer --> patch merging (pooling)\n",
    "    3. Upsampling: concatenate --> swin-transfprmer --> patch expanding (unpooling)\n",
    "    4. Model head\n",
    "    \n",
    "    '''\n",
    "    # Compute number be patches to be embeded\n",
    "    input_size = input_tensor.shape.as_list()[1:]\n",
    "    print(\"input_size\",input_size)\n",
    "    num_patch_x = input_size[0]//patch_size[0]\n",
    "    print(\"num_patch_x\",num_patch_x)\n",
    "    num_patch_y = input_size[1]//patch_size[1]\n",
    "    \n",
    "    # Number of Embedded dimensions\n",
    "    embed_dim = filter_num_begin\n",
    "    \n",
    "    depth_ = depth\n",
    "    \n",
    "    X_skip = []\n",
    "\n",
    "    X = input_tensor\n",
    "    \n",
    "    # Patch extraction\n",
    "    X = transformer_layers.patch_extract(patch_size)(X)\n",
    "\n",
    "    # Embed patches to tokens\n",
    "    X = transformer_layers.patch_embedding(num_patch_x*num_patch_y, embed_dim)(X)\n",
    "    \n",
    "    # The first Swin Transformer stack\n",
    "    X = swin_transformer_stack(X, \n",
    "                               stack_num=stack_num_down, \n",
    "                               embed_dim=embed_dim, \n",
    "                               num_patch=(num_patch_x, num_patch_y), \n",
    "                               num_heads=num_heads[0], \n",
    "                               window_size=window_size[0], \n",
    "                               num_mlp=num_mlp, \n",
    "                               shift_window=shift_window, \n",
    "                               name='{}_swin_down0'.format(name))\n",
    "    X_skip.append(X)\n",
    "    \n",
    "    # Downsampling blocks\n",
    "    for i in range(depth_-1):\n",
    "        \n",
    "        # Patch merging\n",
    "        X = transformer_layers.patch_merging((num_patch_x, num_patch_y), embed_dim=embed_dim, name='down{}'.format(i))(X)\n",
    "        \n",
    "        # update token shape info\n",
    "        embed_dim = embed_dim*2\n",
    "        num_patch_x = num_patch_x//2\n",
    "        num_patch_y = num_patch_y//2\n",
    "        \n",
    "        # Swin Transformer stacks\n",
    "        X = swin_transformer_stack(X, \n",
    "                                   stack_num=stack_num_down, \n",
    "                                   embed_dim=embed_dim, \n",
    "                                   num_patch=(num_patch_x, num_patch_y), \n",
    "                                   num_heads=num_heads[i+1], \n",
    "                                   window_size=window_size[i+1], \n",
    "                                   num_mlp=num_mlp, \n",
    "                                   shift_window=shift_window, \n",
    "                                   name='{}_swin_down{}'.format(name, i+1))\n",
    "        \n",
    "        # Store tensors for concat\n",
    "        X_skip.append(X)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return X, X_skip, embed_dim, num_patch_x, num_patch_y\n",
    "\n",
    "def bottleneck(inputs):\n",
    "    x = inputs\n",
    "    f = 256\n",
    "    \n",
    "    x3 = double_conv_layer(x, 3, f, 0.1, True)\n",
    "    \n",
    "    return x3\n",
    "\n",
    "def decoder(inputs, skip_connections):\n",
    "    num_filters = [128, 64, 32, 16]\n",
    "    skip_connections.reverse()\n",
    "    x = inputs\n",
    "    batch_norm = True\n",
    "    \n",
    "    for i, f in enumerate(num_filters):\n",
    "        \n",
    "        x_up = UpSampling2D(size=(2, 2), data_format=\"channels_last\")(x)\n",
    "        x_att = concatenate([x_up, skip_connections[i]], axis=-1)\n",
    "        \n",
    "        x = double_conv_layer(x_att, 3, f, 0.1, True)\n",
    "    return x\n",
    "\n",
    "def s_decoder(embed_dim,num_patch_x,num_patch_y, X_skip,filter_num_begin, depth, stack_num_down, stack_num_up, \n",
    "                      patch_size, num_heads, window_size, num_mlp, shift_window=True,name='swin_decoder'):\n",
    "    #input_size = [256,256,3]\n",
    "    #num_patch_x = input_size[0]//patch_size[0]\n",
    "    #print(\"numPatchX\",num_patch_x)\n",
    "    #num_patch_y = input_size[1]//patch_size[1]\n",
    "    #print(\"numPatchY\",num_patch_y)\n",
    "    # Number of Embedded dimensions\n",
    "    #embed_dim = filter_num_begin\n",
    "    \n",
    "    depth_ = depth\n",
    "    # reverse indexing encoded tensors and hyperparams\n",
    "    X_skip = X_skip[::-1]\n",
    "    num_heads = num_heads[::-1]\n",
    "    window_size = window_size[::-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # upsampling begins at the deepest available tensor\n",
    "    X = X_skip[0]\n",
    "    print(\"X_skip0\",X)\n",
    "    # other tensors are preserved for concatenation\n",
    "    X_decode = X_skip[1:]\n",
    "    print(\"X-decode\", X_decode)\n",
    "    \n",
    "    depth_decode = len(X_decode)\n",
    "    print(\"depth_decode nnnnnnnnnnnn\",depth_decode)\n",
    "    for i in range(depth_decode):\n",
    "        \n",
    "        # Patch expanding\n",
    "        X = transformer_layers.patch_expanding(num_patch=(num_patch_x, num_patch_y), \n",
    "                                               embed_dim=embed_dim, \n",
    "                                               upsample_rate=2, \n",
    "                                               return_vector=True)(X)\n",
    "        \n",
    "\n",
    "        # update token shape info\n",
    "        embed_dim = embed_dim//2\n",
    "        num_patch_x = num_patch_x*2\n",
    "        num_patch_y = num_patch_y*2\n",
    "        \n",
    "        # Concatenation and linear projection\n",
    "        X = concatenate([X, X_decode[i]], axis=-1, name='{}_concat_{}'.format(name, i))\n",
    "        X = Dense(embed_dim, use_bias=False, name='{}_concat_linear_proj_{}'.format(name, i))(X)\n",
    "        \n",
    "        # Swin Transformer stacks\n",
    "        X = swin_transformer_stack(X, \n",
    "                                   stack_num=stack_num_up, \n",
    "                                   embed_dim=embed_dim, \n",
    "                                   num_patch=(num_patch_x, num_patch_y), \n",
    "                                   num_heads=num_heads[i], \n",
    "                                   window_size=window_size[i], \n",
    "                                   num_mlp=num_mlp, \n",
    "                                   shift_window=shift_window, \n",
    "                                   name='{}_swin_up{}'.format(name, i))\n",
    "        \n",
    "    # The last expanding layer; it produces full-size feature maps based on the patch size\n",
    "    # !!! <--- \"patch_size[0]\" is used; it assumes patch_size = (size, size)\n",
    "    \n",
    "    X = transformer_layers.patch_expanding(num_patch=(num_patch_x, num_patch_y), \n",
    "                                           embed_dim=embed_dim, \n",
    "                                           upsample_rate=patch_size[0], \n",
    "                                           return_vector=False)(X)\n",
    "    \n",
    "    return X\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def output(inputs):\n",
    "    x = Conv2D(1, kernel_size=(1,1))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b285d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_num_begin = 128     # number of channels in the first downsampling block; it is also the number of embedded dimensions\n",
    "depth = 4               # the depth of SwinUNET; depth=4 means three down/upsampling levels and a bottom level \n",
    "stack_num_down = 2         # number of Swin Transformers per downsampling level\n",
    "stack_num_up = 2           # number of Swin Transformers per upsampling level\n",
    "patch_size = (4, 4)        # Extract 4-by-4 patches from the input image. Height and width of the patch must be equal.\n",
    "num_heads = [4, 8, 8, 8]   # number of attention heads per down/upsampling level\n",
    "window_size = [4, 2, 2, 2] # the size of attention window per down/upsampling level\n",
    "num_mlp = 512              # number of MLP nodes within the Transformer\n",
    "shift_window=True          # Apply window shifting, i.e., Swin-MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7aef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f680a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_resize_crop(image, scale=[0.75, 1.0], crop_size=128):\n",
    "    if crop_size == 32:\n",
    "        image_shape = 48\n",
    "        image = tf.image.resize(image, (image_shape, image_shape))\n",
    "    else:\n",
    "        image_shape = 96\n",
    "        image = tf.image.resize(image, (image_shape, image_shape))\n",
    "    size = tf.random.uniform(\n",
    "        shape=(1,),\n",
    "        minval=scale[0] * image_shape,\n",
    "        maxval=scale[1] * image_shape,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    size = tf.cast(size, tf.int32)[0]\n",
    "    crop = tf.image.random_crop(image, (size, size, 3))\n",
    "    crop_resize = tf.image.resize(crop, (crop_size, crop_size))\n",
    "    return crop_resize\n",
    "\n",
    "def flip_random_crop(image):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = random_resize_crop(image, crop_size=CROP_TO)\n",
    "    return image\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def float_parameter(level, maxval):\n",
    "    return tf.cast(level * maxval / 10.0, tf.float32)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def sample_level(n):\n",
    "    return tf.random.uniform(shape=[1], minval=0.1, maxval=n, dtype=tf.float32)\n",
    "\n",
    "def rotation(image):\n",
    "    augmented_image = tf.image.rot90(image)\n",
    "    return augmented_image\n",
    "\n",
    "@tf.function\n",
    "def solarize(image, level=6):\n",
    "    threshold = float_parameter(sample_level(level), 1)\n",
    "    return tf.where(image < threshold, image, 255 - image)\n",
    "\n",
    "def color_jitter(x, strength=0.5):\n",
    "    x = tf.image.random_brightness(x, max_delta=0.8 * strength)\n",
    "    x = tf.image.random_contrast(\n",
    "        x, lower=1 - 0.8 * strength, upper=1 + 0.8 * strength\n",
    "    )\n",
    "    x = tf.image.random_saturation(\n",
    "        x, lower=1 - 0.8 * strength, upper=1 + 0.8 * strength\n",
    "    )\n",
    "    x = tf.image.random_hue(x, max_delta=0.2 * strength)\n",
    "    x = tf.clip_by_value(x, 0, 255)\n",
    "    return x\n",
    "\n",
    "\n",
    "def color_drop(x):\n",
    "    x = tf.image.rgb_to_grayscale(x)\n",
    "    x = tf.tile(x, [1, 1, 3])\n",
    "    return x\n",
    "\n",
    "\n",
    "def random_apply(func, x, p):\n",
    "    if tf.random.uniform([], minval=0, maxval=1) < p:\n",
    "        return func(x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def custom_augment(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = flip_random_crop(image)\n",
    "    image = random_apply(rotation, image, p=0.5)\n",
    "    #image = random_apply(color_jitter, image, p=0.9)\n",
    "    #image = random_apply(color_drop, image, p=0.3)\n",
    "    #image = random_apply(solarize, image, p=0.3)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bee557",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "CROP_TO = IMG_HEIGHT\n",
    "SEED = 42\n",
    "BATCH_SIZE = 4\n",
    "ssl_ds_one = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "ssl_ds_one = (\n",
    "    ssl_ds_one.shuffle(1024, seed=SEED)\n",
    "    .map(custom_augment, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "ssl_ds_two = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "ssl_ds_two = (\n",
    "    ssl_ds_two.shuffle(1024, seed=SEED)\n",
    "    .map(custom_augment, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "# We then zip both of these datasets.\n",
    "ssl_ds = tf.data.Dataset.zip((ssl_ds_one, ssl_ds_two))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3781c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images_one = next(iter(ssl_ds_one))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for n in range(2):\n",
    "    ax = plt.subplot(5, 5, n + 1)\n",
    "    plt.imshow(sample_images_one[n].numpy().astype(\"int\"))\n",
    "    #print(sample_images_one[n].numpy().shape)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dcb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images_two = next(iter(ssl_ds_two))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for n in range(2):\n",
    "    ax = plt.subplot(5, 5, n + 1)\n",
    "    plt.imshow(sample_images_two[n].numpy().astype(\"int\"))\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b34d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_diagonal(x):\n",
    "    n = tf.shape(x)[0]\n",
    "    flattened = tf.reshape(x, [-1])[:-1]\n",
    "    off_diagonals = tf.reshape(flattened, (n-1, n+1))[:, 1:]\n",
    "    return tf.reshape(off_diagonals, [-1])\n",
    "\n",
    "\n",
    "def normalize_repr(z):\n",
    "    z_norm = (z - tf.reduce_mean(z, axis=0)) / tf.math.reduce_std(z, axis=0)\n",
    "    return z_norm\n",
    "\n",
    "\n",
    "def compute_loss(z_a, z_b, lambd):\n",
    "    # Get batch size and representation dimension.\n",
    "    batch_size = tf.cast(tf.shape(z_a)[0], z_a.dtype)\n",
    "    repr_dim = tf.shape(z_a)[1]\n",
    "\n",
    "    # Normalize the representations along the batch dimension.\n",
    "    z_a_norm = normalize_repr(z_a)\n",
    "    z_b_norm = normalize_repr(z_b)\n",
    "\n",
    "    # Cross-correlation matrix.\n",
    "    c = tf.matmul(z_a_norm, z_b_norm, transpose_a=True) / batch_size\n",
    "\n",
    "    # Loss.\n",
    "    on_diag = tf.linalg.diag_part(c) + (-1)\n",
    "    on_diag = tf.reduce_sum(tf.pow(on_diag, 2))\n",
    "    off_diag = off_diagonal(c)\n",
    "    off_diag = tf.reduce_sum(tf.pow(off_diag, 2))\n",
    "    loss = on_diag + (lambd * off_diag)\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33914ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarlowTwins(tf.keras.Model):\n",
    "    def __init__(self, encoder, lambd=5e-3):\n",
    "        super(BarlowTwins, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.lambd = lambd\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        ds_one, ds_two = data\n",
    "\n",
    "        # Forward pass through the encoder and predictor.\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_a, z_b = self.encoder(ds_one, training=True), self.encoder(ds_two, training=True)\n",
    "            loss = compute_loss(z_a, z_b, self.lambd) \n",
    "\n",
    "        # Compute gradients and update the parameters.\n",
    "        gradients = tape.gradient(loss, self.encoder.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.encoder.trainable_variables))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8b79a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Implements an LR scheduler that warms up the learning rate for some training steps\n",
    "    (usually at the beginning of the training) and then decays it\n",
    "    with CosineDecay (see https://arxiv.org/abs/1608.03983)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
    "    ):\n",
    "        super(WarmUpCosine, self).__init__()\n",
    "\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.pi = tf.constant(np.pi)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.total_steps < self.warmup_steps:\n",
    "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
    "        learning_rate = (\n",
    "            0.5\n",
    "            * self.learning_rate_base\n",
    "            * (\n",
    "                1\n",
    "                + tf.cos(\n",
    "                    self.pi\n",
    "                    * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
    "                    / float(self.total_steps - self.warmup_steps)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if self.warmup_steps > 0:\n",
    "            if self.learning_rate_base < self.warmup_learning_rate:\n",
    "                raise ValueError(\n",
    "                    \"Learning_rate_base must be larger or equal to \"\n",
    "                    \"warmup_learning_rate.\"\n",
    "                )\n",
    "            slope = (\n",
    "                self.learning_rate_base - self.warmup_learning_rate\n",
    "            ) / self.warmup_steps\n",
    "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps, warmup_rate, learning_rate\n",
    "            )\n",
    "        return tf.where(\n",
    "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f4864",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIM = IMG_HEIGHT/2\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100\n",
    "WEIGHT_DECAY = 5e-4\n",
    "TRAIN_FLG = 1 # 0 - No Training, 1 - Training\n",
    "val_split = 0.3\n",
    "\n",
    "STEPS_PER_EPOCH = len(X_train) // BATCH_SIZE\n",
    "TOTAL_STEPS = STEPS_PER_EPOCH * EPOCHS\n",
    "WARMUP_EPOCHS = int(EPOCHS * 0.1)\n",
    "WARMUP_STEPS = int(WARMUP_EPOCHS * STEPS_PER_EPOCH)\n",
    "\n",
    "lr_decayed_fn = WarmUpCosine(\n",
    "    learning_rate_base=1e-3,\n",
    "    total_steps=EPOCHS * STEPS_PER_EPOCH,\n",
    "    warmup_learning_rate=0.0,\n",
    "    warmup_steps=WARMUP_STEPS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the LR schedule\n",
    "plt.plot(lr_decayed_fn(tf.range(EPOCHS*STEPS_PER_EPOCH, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16acd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_head(x, hidden_dim=128):\n",
    "    \"\"\"Constructs the projection head.\"\"\"\n",
    "    for i in range(2):\n",
    "        x = Dense(\n",
    "            hidden_dim,\n",
    "            name=f\"projection_layer_{i}\",\n",
    "            kernel_regularizer=l2(WEIGHT_DECAY),\n",
    "        )(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "    outputs = Dense(hidden_dim, name=\"projection_output\")(x)\n",
    "    return outputs\n",
    "\n",
    "def build_encoder(shape, hidden_dim=128):\n",
    "    inputs = Input(shape)\n",
    "    s = layers.experimental.preprocessing.Rescaling(1.0 / 255)(inputs)\n",
    "    #s = inputs\n",
    "    x, skip_1,embed_dim_1,num_patch_x_1,num_patch_y_1 = encoder(s,filter_num_begin, depth, stack_num_down, stack_num_up, \n",
    "                      patch_size, num_heads, window_size, num_mlp, shift_window=True)\n",
    "    \n",
    "    #x = bottleneck(x)\n",
    "    \n",
    "    # Projections\n",
    "    trunk_output = Flatten()(x)\n",
    "    print(\"trunk_output\",trunk_output)\n",
    "    projection_outputs = projection_head(trunk_output, hidden_dim=hidden_dim)\n",
    "\n",
    "    model = Model(inputs, projection_outputs)\n",
    "    return model, skip_1,embed_dim_1,num_patch_x_1,num_patch_y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3b7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_enc, skip_connetions,embed_dim_1,num_patch_x_1,num_patch_y_1= build_encoder((IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS), hidden_dim=PROJECT_DIM)\n",
    "unet_enc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f68d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_decayed_fn, momentum=0.9)\n",
    "barlow_twins = BarlowTwins(unet_enc)\n",
    "barlow_twins.compile(optimizer=optimizer)\n",
    "if TRAIN_FLG:\n",
    "    print(\"Training\")\n",
    "    barlow_twins.encoder.get_weights()[0]\n",
    "    history = barlow_twins.fit(ssl_ds, epochs=200)\n",
    "    barlow_twins.encoder.save('barlow_twins_unet')\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.grid()\n",
    "    plt.title(\"Barlow Twin Loss\")\n",
    "    plt.show()\n",
    "elif not TRAIN_FLG:\n",
    "    print(\"Loading training weights\")\n",
    "    barlow_twins.encoder.load_weights('barlow_twins_unet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d635b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ae6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = tf.keras.Model(\n",
    "    barlow_twins.encoder.input, barlow_twins.encoder.layers[-9].output\n",
    ")\n",
    "backbone.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b21e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26069bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_connetions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_skip_connections = [backbone.get_layer(index=5).output, \n",
    "                        backbone.get_layer(index=8).output, \n",
    "                        backbone.get_layer(index=11).output, \n",
    "                        backbone.get_layer(index=14).output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_skip_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_skip_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf2b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.trainable=True\n",
    "#x = backbone.output\n",
    "#x = bottleneck(x)\n",
    "x = s_decoder( embed_dim_1,num_patch_x_1,num_patch_y_1,skip_connetions,filter_num_begin, depth, stack_num_down, stack_num_up, \n",
    "                      patch_size, num_heads, window_size, num_mlp, shift_window=True)\n",
    "outputs = output(x)\n",
    "model = Model(barlow_twins.encoder.input, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554712d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4810251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = label(y_true_in > 0.5)\n",
    "    y_pred = label(y_pred_in > 0.5)\n",
    "    \n",
    "    true_objects = len(np.unique(labels))\n",
    "    pred_objects = len(np.unique(y_pred))\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "    #print('IOU {}'.format(iou))\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    value = 0.\n",
    "    for batch in range(batch_size):\n",
    "        value = value + iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "    return value/batch_size\n",
    "\n",
    "def my_iou_metric(label, pred):\n",
    "    metric_value = tf.py_function(iou_metric_batch, [label, pred], tf.float32)\n",
    "    return metric_value\n",
    "\n",
    "def my_iou_metric_loss(label, pred):\n",
    "    loss = 1-tf.py_function(iou_metric_batch, [label, pred], tf.float32)\n",
    "    #loss = -tf.map_fn(my_iou_metric_loss(label, pred), tf.range(tf.shape(pred)[0]))\n",
    "    loss.set_shape((None,))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def haud_dist(y_true, y_pred):\n",
    "    y_true = np.squeeze(y_true)\n",
    "    y_pred = np.squeeze(y_pred)\n",
    "    \n",
    "    return hausdorff_distance(y_true,y_pred)\n",
    "\n",
    "def haud_dist_batch(y_true, y_pred):\n",
    "    if len(y_true.shape)==2:\n",
    "        return haud_dist(y_true, y_pred)\n",
    "    else:\n",
    "        batch_size = y_true.shape[0]\n",
    "    hd = 0.\n",
    "    for batch in range(batch_size):\n",
    "        hd = hd + haud_dist(y_true[batch], y_pred[batch])\n",
    "    return hd/batch_size\n",
    "\n",
    "def my_haud_dist(label, pred):\n",
    "    metric_value = tf.py_function(haud_dist_batch, [label, pred], tf.float32)\n",
    "    return metric_value\n",
    "\n",
    "def evalResult(gt,pred,target_size=(256,256),flag_multi_class = False,num_class = 2):\n",
    "    gt = np.squeeze(gt)\n",
    "    pred = np.squeeze(pred)\n",
    "    \n",
    "    acc = Accuracy()\n",
    "    acc.update_state(np.squeeze(gt), np.squeeze(pred))\n",
    "    r_acc = acc.result().numpy()\n",
    "    \n",
    "    pr = Precision()\n",
    "    pr.update_state(np.squeeze(gt), np.squeeze(pred))\n",
    "    r_pr = pr.result().numpy()\n",
    "    \n",
    "    rc = Recall()\n",
    "    rc.update_state(np.squeeze(gt), np.squeeze(pred))\n",
    "    r_rc = rc.result().numpy()\n",
    "    \n",
    "    mi = MeanIoU(num_class)\n",
    "    mi.update_state(np.squeeze(gt), np.squeeze(pred))\n",
    "    r_mi = mi.result().numpy()\n",
    "    \n",
    "    dc = 0.\n",
    "    for img in range(gt.shape[0]):\n",
    "        dc = dc + dice_coeff(gt[img], pred[img]).numpy()\n",
    "    dc = dc / gt.shape[0]\n",
    "    \n",
    "    hd = haud_dist_batch(gt,pred)\n",
    "    \n",
    "    miou = iou_metric_batch(gt,pred)\n",
    "    \n",
    "    mae = MeanAbsoluteError()\n",
    "    r_mae = mae(np.squeeze(gt), np.squeeze(pred)).numpy()\n",
    "\n",
    "    print(\"Accuracy=\",r_acc, \"Precision=\",r_pr, \"Recall=\",r_rc, \"MeanIoU=\",r_mi, \"DiceCoefficient=\",dc, \"HD=\",hd, \"MyIoU=\",miou, \"MAE=\",r_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43fc934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(target_tensor, prediction_tensor, weights=None, alpha=0.25, gamma=2):\n",
    "    sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.math.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0))                           - (1 - alpha) * (neg_p_sub ** gamma) * tf.math.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)\n",
    "\n",
    "def mean_iou_loss(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.compat.v1.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.compat.v1.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return -tf.math.log(K.mean(K.stack(prec), axis=0))\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = 0.4*categorical_crossentropy(y_true, y_pred) + 0.6*dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def bce_dice_loss2(y_true, y_pred):\n",
    "    fl = focal_loss(y_true, y_pred, gamma=5)\n",
    "    loss = 0.2*categorical_crossentropy(y_true, y_pred) + 0.3*dice_loss(y_true, y_pred) + 0.5*fl\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3f6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"logs/\"\n",
    "keyname = \"BT-Unet\"\n",
    "cur_date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tb_log_dir = log_path + \"fit/\" + keyname + '_' + cur_date \n",
    "tensorboard_callback = TensorBoard(log_dir=tb_log_dir, histogram_freq=0)\n",
    "model_checkpoint = ModelCheckpoint('model_'+keyname+'.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='loss', verbose=1, patience=20)\n",
    "csv_logger = CSVLogger(log_path + keyname + '_' + cur_date + '.log', separator=',', append=False)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=bce_dice_loss,\n",
    "    optimizer=Adam(),\n",
    "    metrics=['accuracy', Precision(), MeanIoU(num_classes=2), Recall(), dice_coeff, MeanAbsoluteError(), my_haud_dist, my_iou_metric]\n",
    "   )\n",
    "if TRAIN_FLG:\n",
    "    print(\"Training\")\n",
    "    callbacks = [\n",
    "        model_checkpoint,\n",
    "        reduce_lr,\n",
    "        csv_logger,\n",
    "        tensorboard_callback,\n",
    "        early_stopping\n",
    "    ]\n",
    "    X_train_20 = X_train[:int(X_train.shape[0]*0.2)]\n",
    "    Y_train_20 = Y_train[:int(Y_train.shape[0]*0.2)]\n",
    "    results = model.fit(X_train_20, Y_train_20, validation_split=val_split, batch_size=8, epochs=400)\n",
    "elif not TRAIN_FLG:\n",
    "    print(\"Loading training weights\")\n",
    "    model.load_weights('model_'+keyname+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f825d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = model.predict(X_train_20[:int(X_train_20.shape[0]*(1-val_split))], verbose=1)\n",
    "preds_x = model.predict(X_train, verbose=1)\n",
    "preds_val = model.predict(X_train_20[int(X_train_20.shape[0]*(1-val_split)):], verbose=1)\n",
    "preds_test = model.predict(X_test, verbose=1)\n",
    "\n",
    "gt_train = Y_train_20[:int(X_train_20.shape[0]*(1-val_split))].astype(np.float32)\n",
    "gt_x = Y_train.astype(np.float32)\n",
    "gt_val = Y_train_20[int(X_train.shape[0]*(1-val_split)):].astype(np.float32)\n",
    "gt_test = Y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold predictions\n",
    "preds_train_t = (preds_train > 0.5).astype(np.float32)\n",
    "preds_x_t = (preds_x > 0.5).astype(np.float32)\n",
    "preds_val_t = (preds_val > 0.5).astype(np.float32)\n",
    "preds_test_t = (preds_test > 0.5).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ed80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalResult(gt_train, preds_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d2b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalResult(gt_x, preds_x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ddd5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalResult(gt_test, preds_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d3bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "\n",
    "xx = [1,0,0]\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    #ix = random.randint(0, len(preds_train))\n",
    "    ix = xx[i]\n",
    "    #print(ix)\n",
    "    #ix = 58\n",
    "    plt.subplot(3,3,x+1)\n",
    "    imshow(X_train[:int(X_train.shape[0]*(1-val_split))][ix].astype('uint8'))\n",
    "    #plt.title('Image')\n",
    "\n",
    "    plt.subplot(3,3,x+2)\n",
    "    imshow(np.squeeze(Y_train[:int(Y_train.shape[0]*(1-val_split))][ix]))\n",
    "    #plt.title('Mask')\n",
    "\n",
    "    plt.subplot(3,3,x+3)\n",
    "    imshow(np.squeeze(preds_train_t[ix]))\n",
    "    #plt.title('Predicted Mask')\n",
    "    #plt.show()\n",
    "    \n",
    "    plt.show()\n",
    "    evalResult(Y_train[:int(Y_train.shape[0]*(1-val_split))][ix], preds_train_t[ix])\n",
    "    x = x+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d809ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "xx = [16,10,15]\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    #ix = random.randint(0, len(preds_train))\n",
    "    ix = xx[i]\n",
    "    #print(ix)\n",
    "    #ix = 58\n",
    "    plt.subplot(3,3,x+1)\n",
    "    imshow(X_test[ix].astype('uint8'))\n",
    "    #plt.title('Image')\n",
    "\n",
    "    plt.subplot(3,3,x+2)\n",
    "    imshow(np.squeeze(Y_test[ix]))\n",
    "    #plt.title('Mask')\n",
    "\n",
    "    plt.subplot(3,3,x+3)\n",
    "    imshow(np.squeeze(preds_test_t[ix]))\n",
    "    #plt.title('Predicted Mask')\n",
    "    #plt.show()\n",
    "    \n",
    "    plt.show()\n",
    "    evalResult(Y_test[ix], preds_test_t[ix])\n",
    "    x = x+3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbef243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
